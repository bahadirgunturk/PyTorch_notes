{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIzf09RxXY41"
      },
      "source": [
        "**Transfer Learning**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sz_23M5skjfX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "learning_rate = 1e-2\n",
        "\n",
        "# image transform\n",
        "transform1 = transforms.Compose([transforms.Resize(64),\n",
        "                                 transforms.Grayscale(3), #return 3 channels \n",
        "                                 transforms.ToTensor()])\n",
        "\n",
        "# download train dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./mnist_data/',\n",
        "                                           train=True,\n",
        "                                           transform=transform1,\n",
        "                                           download=True)\n",
        "\n",
        "# download test dataset\n",
        "test_dataset = torchvision.datasets.MNIST(root='./mnist_data/',\n",
        "                                           train=False,\n",
        "                                           transform=transform1,\n",
        "                                           download=True)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader =  torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to train the model for one epoch\n",
        "def train_model(model, optimizer, loss_function, train_loader):\n",
        "  \n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # For loss calculation\n",
        "  total_loss = 0\n",
        "  total_correct = 0\n",
        "  dataset_size = len(train_loader.dataset)\n",
        "\n",
        "  # Go over each batch\n",
        "  for X, y in train_loader:\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_function(y_pred,y)  \n",
        "    # In the above\n",
        "    #   input:y_pred is logits, target y is integer index.\n",
        "    #   target could also be a vector of probabilities\n",
        "    #     in that case, y.softmax(1) could be used to convert to probabilities  \n",
        "    \n",
        "    # Zero the gradients, backpropagate the gradients, update the parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate loss\n",
        "    total_loss += loss_function(y_pred, y).item()\n",
        "    total_correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  print(f\"Training Loss:{total_loss/dataset_size:0.4f}, \\\n",
        "          Training Accuracy:{total_correct/dataset_size:0.4f}\")\n",
        "\n",
        "\n",
        "# Define a function to test the model\n",
        "def test_model(model, loss_function, test_loader):\n",
        "  # Put the model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # For loss calculation\n",
        "  total_loss = 0\n",
        "  total_correct = 0\n",
        "  dataset_size = len(test_loader.dataset)\n",
        "\n",
        "  # Do not calculate gradients\n",
        "  with torch.no_grad():\n",
        "    for X,y in test_loader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Calculate loss\n",
        "      total_loss += loss_function(y_pred, y).item()\n",
        "      total_correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  \n",
        "  print(f\"Test Loss: {total_loss/dataset_size:0.4f}, \\\n",
        "          Test Accuracy: {total_correct/dataset_size:0.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "-4-4TKzJVlzq",
        "outputId": "86eafd7e-d027-40f1-c219-f1fcf11055c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./mnist_data/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=64, interpolation=bilinear, max_size=None, antialias=warn)\n",
            "               Grayscale(num_output_channels=3)\n",
            "               ToTensor()\n",
            "           )\n",
            "torch.Size([3, 64, 64])\n",
            "5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '8')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check out images in the dataset\n",
        "print(train_dataset)\n",
        "#print(train_dataset[0]) # is a tuple of (data,target)\n",
        "print(train_dataset[0][0].shape)  # image (a tensor)\n",
        "print(train_dataset[0][1])        # label (an integer)\n",
        "\n",
        "\n",
        "#print(train_dataset.data.shape)   # all images in the dataset\n",
        "\n",
        "# Look at a sample\n",
        "idx = 0\n",
        "image0 = train_dataset.data[idx]\n",
        "label0 = train_dataset.targets[idx]\n",
        "\n",
        "# The above image0 is of size [28,28], not yet transformed\n",
        "# Dataloader will do the transform and output it in batches\n",
        "image, label = next(iter(train_loader))\n",
        "# The image is of size [128,3,64,64]\n",
        "\n",
        "# To display a single image\n",
        "image1 = image[0].squeeze()     # get the 0th image in batch\n",
        "image1 = image1.permute(1,2,0)  # channel goes to the end\n",
        "image1 = image1.numpy()         # from tensor to numpy\n",
        "label1 = label[0].item()\n",
        "\n",
        "plt.imshow(image1)\n",
        "plt.title(label1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHqhd9g2aq1F"
      },
      "source": [
        "Download a model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyo0byJB3wPa",
        "outputId": "7a4f7b35-0a21-4181-cd71-f3ef1e1b5596"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bahadir/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/bahadir/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /home/bahadir/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [04:16<00:00, 2.24MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bahadir/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
            "/home/bahadir/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return super().__sizeof__() + self.nbytes()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                   Kernel Shape              Output Shape              Param #\n",
              "===================================================================================================================\n",
              "VGG                                      --                        [1, 1000]                 --\n",
              "├─Sequential: 1-1                        --                        [1, 512, 1, 1]            --\n",
              "│    └─Conv2d: 2-1                       [3, 3]                    [1, 64, 32, 32]           1,792\n",
              "│    └─ReLU: 2-2                         --                        [1, 64, 32, 32]           --\n",
              "│    └─Conv2d: 2-3                       [3, 3]                    [1, 64, 32, 32]           36,928\n",
              "│    └─ReLU: 2-4                         --                        [1, 64, 32, 32]           --\n",
              "│    └─MaxPool2d: 2-5                    2                         [1, 64, 16, 16]           --\n",
              "│    └─Conv2d: 2-6                       [3, 3]                    [1, 128, 16, 16]          73,856\n",
              "│    └─ReLU: 2-7                         --                        [1, 128, 16, 16]          --\n",
              "│    └─Conv2d: 2-8                       [3, 3]                    [1, 128, 16, 16]          147,584\n",
              "│    └─ReLU: 2-9                         --                        [1, 128, 16, 16]          --\n",
              "│    └─MaxPool2d: 2-10                   2                         [1, 128, 8, 8]            --\n",
              "│    └─Conv2d: 2-11                      [3, 3]                    [1, 256, 8, 8]            295,168\n",
              "│    └─ReLU: 2-12                        --                        [1, 256, 8, 8]            --\n",
              "│    └─Conv2d: 2-13                      [3, 3]                    [1, 256, 8, 8]            590,080\n",
              "│    └─ReLU: 2-14                        --                        [1, 256, 8, 8]            --\n",
              "│    └─Conv2d: 2-15                      [3, 3]                    [1, 256, 8, 8]            590,080\n",
              "│    └─ReLU: 2-16                        --                        [1, 256, 8, 8]            --\n",
              "│    └─Conv2d: 2-17                      [3, 3]                    [1, 256, 8, 8]            590,080\n",
              "│    └─ReLU: 2-18                        --                        [1, 256, 8, 8]            --\n",
              "│    └─MaxPool2d: 2-19                   2                         [1, 256, 4, 4]            --\n",
              "│    └─Conv2d: 2-20                      [3, 3]                    [1, 512, 4, 4]            1,180,160\n",
              "│    └─ReLU: 2-21                        --                        [1, 512, 4, 4]            --\n",
              "│    └─Conv2d: 2-22                      [3, 3]                    [1, 512, 4, 4]            2,359,808\n",
              "│    └─ReLU: 2-23                        --                        [1, 512, 4, 4]            --\n",
              "│    └─Conv2d: 2-24                      [3, 3]                    [1, 512, 4, 4]            2,359,808\n",
              "│    └─ReLU: 2-25                        --                        [1, 512, 4, 4]            --\n",
              "│    └─Conv2d: 2-26                      [3, 3]                    [1, 512, 4, 4]            2,359,808\n",
              "│    └─ReLU: 2-27                        --                        [1, 512, 4, 4]            --\n",
              "│    └─MaxPool2d: 2-28                   2                         [1, 512, 2, 2]            --\n",
              "│    └─Conv2d: 2-29                      [3, 3]                    [1, 512, 2, 2]            2,359,808\n",
              "│    └─ReLU: 2-30                        --                        [1, 512, 2, 2]            --\n",
              "│    └─Conv2d: 2-31                      [3, 3]                    [1, 512, 2, 2]            2,359,808\n",
              "│    └─ReLU: 2-32                        --                        [1, 512, 2, 2]            --\n",
              "│    └─Conv2d: 2-33                      [3, 3]                    [1, 512, 2, 2]            2,359,808\n",
              "│    └─ReLU: 2-34                        --                        [1, 512, 2, 2]            --\n",
              "│    └─Conv2d: 2-35                      [3, 3]                    [1, 512, 2, 2]            2,359,808\n",
              "│    └─ReLU: 2-36                        --                        [1, 512, 2, 2]            --\n",
              "│    └─MaxPool2d: 2-37                   2                         [1, 512, 1, 1]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 --                        [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        --                        [1, 1000]                 --\n",
              "│    └─Linear: 2-38                      --                        [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-39                        --                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-40                     --                        [1, 4096]                 --\n",
              "│    └─Linear: 2-41                      --                        [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-42                        --                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-43                     --                        [1, 4096]                 --\n",
              "│    └─Linear: 2-44                      --                        [1, 1000]                 4,097,000\n",
              "===================================================================================================================\n",
              "Total params: 143,667,240\n",
              "Trainable params: 143,667,240\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 522.08\n",
              "===================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 2.50\n",
              "Params size (MB): 574.67\n",
              "Estimated Total Size (MB): 577.18\n",
              "==================================================================================================================="
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model1 = torchvision.models.vgg19(pretrained=True).to(device)\n",
        "print(model1)\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(model1, (1, 3, 32,32), col_names=[\"kernel_size\", \"output_size\", \"num_params\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipgpbuc8eKk6",
        "outputId": "4e14f2b8-c458-4a88-b05a-a53b8585c903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            ")\n",
            "Linear(in_features=25088, out_features=4096, bias=True)\n",
            "<bound method Module.parameters of Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            ")>\n"
          ]
        }
      ],
      "source": [
        "# You can access the modules/layers inside model\n",
        "#print(model1)\n",
        "#print(model1.features)\n",
        "#print(model1.avgpool)\n",
        "print(model1.classifier)\n",
        "print(model1.classifier[0])\n",
        "print(model1.classifier.parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXfBDjxC0xf-",
        "outputId": "fe8d2500-e3bc-4e45-a56a-0e869776cef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): new_block(\n",
            "    (linear1): Linear(in_features=25088, out_features=100, bias=True)\n",
            "    (relu): ReLU()\n",
            "    (dropout1): Dropout(p=0.5, inplace=False)\n",
            "    (linear2): Linear(in_features=100, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get another copy of the model and play with it\n",
        "model2 = torchvision.models.vgg19(pretrained=True).to(device)\n",
        "\n",
        "# Freeze the parameters in the features part\n",
        "for param in model2.features.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Create an identity network; you can replace a part of the network\n",
        "#class identity_block(torch.nn.Module):\n",
        "#  def __init__(self):\n",
        "#    super().__init__()\n",
        "#  def forward(self,x):\n",
        "#    return x\n",
        "# For example, \n",
        "#model2.avgpool = identity_block()\n",
        "# OR\n",
        "#model2.avgpool = torch.nn.Identity()\n",
        "\n",
        "# Replace the last layer of the classifier to have 5 out_features\n",
        "#model2.classifier[6] = nn.Linear(in_features=4096,out_features=5)\n",
        "#print(model2)\n",
        "\n",
        "# Create a new classifier block\n",
        "class new_block(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(in_features=25088, out_features=100)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout()\n",
        "    self.linear2 = nn.Linear(in_features=100,out_features=5)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.linear2(x)\n",
        "    # x = self.dropout1(x) # Adding this would not change print(model) but\n",
        "    #   the forward process would change\n",
        "    #   Note that when you print(model), it does not show flattening...\n",
        "    return x\n",
        "\n",
        "# Replace the existing block\n",
        "model2.classifier = new_block()\n",
        "# OR \n",
        "'''\n",
        "model2.classifier = nn.Sequential(\n",
        "    nn.Linear(in_features=25088, out_features=100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(in_features=100,out_features=5))\n",
        "'''\n",
        "\n",
        "print(model2)\n",
        "\n",
        "# Access a specific layer\n",
        "#print(model2.classifier.linear1)\n",
        "# OR\n",
        "#print(model2.classifier[0])\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PElIZcqu37Ip",
        "outputId": "ff87fb1d-5636-4667-bed3-9e41144ce8ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): ReLU(inplace=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (26): ReLU(inplace=True)\n",
            "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "tensor([[[-0.0535, -0.0493, -0.0679],\n",
            "         [ 0.0153,  0.0451,  0.0021],\n",
            "         [ 0.0362,  0.0200,  0.0199]],\n",
            "\n",
            "        [[ 0.0170,  0.0554, -0.0062],\n",
            "         [ 0.1416,  0.2271,  0.1376],\n",
            "         [ 0.1200,  0.2003,  0.0921]],\n",
            "\n",
            "        [[-0.0449,  0.0127, -0.0145],\n",
            "         [ 0.0597,  0.1395,  0.0541],\n",
            "         [-0.0010,  0.0583, -0.0297]]], device='cuda:0')\n",
            "False\n",
            "tensor(-0.9130, device='cuda:0')\n",
            "False\n",
            "tensor([[[-0.0535, -0.0493, -0.0679],\n",
            "         [ 0.0153,  0.0451,  0.0021],\n",
            "         [ 0.0362,  0.0200,  0.0199]],\n",
            "\n",
            "        [[ 0.0170,  0.0554, -0.0062],\n",
            "         [ 0.1416,  0.2271,  0.1376],\n",
            "         [ 0.1200,  0.2003,  0.0921]],\n",
            "\n",
            "        [[-0.0449,  0.0127, -0.0145],\n",
            "         [ 0.0597,  0.1395,  0.0541],\n",
            "         [-0.0010,  0.0583, -0.0297]]], device='cuda:0')\n",
            "False\n",
            "tensor(-0.9130, device='cuda:0')\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# Take part of a model\n",
        "#   Create a list and unpack it (with *) for nn.Sequential\n",
        "model_part = nn.Sequential(*[model2.features[i] for i in range(31)])\n",
        "print(model_part)\n",
        "\n",
        "# Check out the weights; they should be equal\n",
        "for param in model2.features[0].parameters():\n",
        "  print(param.data[0])\n",
        "  print(param.requires_grad)\n",
        "  \n",
        "\n",
        "for param in model_part[0].parameters():\n",
        "  print(param.data[0])\n",
        "  print(param.requires_grad)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adbcPPYX607A",
        "outputId": "6a3549ba-2f1a-4c88-d5cd-f580a3e021f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 4, 4])\n",
            "Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=8192, out_features=100, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "# Now, pass a random input, check out the dimension of the model_part\n",
        "#image, label = next(iter(train_loader))\n",
        "image = torch.randn((1,3,64,64)).to(device)\n",
        "y = model_part(image)\n",
        "print(y.shape)\n",
        "#>> torch.Size([1, 512, 4, 4])\n",
        "\n",
        "# Using shape info, you can create a new block and append to model_part\n",
        "model_part2 = nn.Sequential(\n",
        "    nn.Flatten(),   \n",
        "    # The above is part of nn.module. Compare with nn.flatten()\n",
        "    # ... If a class is written, in .forward, you can write x.view(-1,512*4*4)\n",
        "    # For vgg19, print(model) doesn't display .forward but the defined layers\n",
        "    nn.Linear(in_features=512*4*4, out_features=100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(in_features=100,out_features=10))\n",
        "\n",
        "#print(model_part2)\n",
        "\n",
        "# Append these blocks\n",
        "model3 = nn.Sequential(model_part,model_part2).to(device)\n",
        "print(model3)\n",
        "#print(model3[0][0])\n",
        "\n",
        "y2 = model3(image)\n",
        "print(y2.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZNLyC75tB5Qv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Training Loss:0.0060,           Training Accuracy:0.7781\n",
            "Test Loss: 0.0019,           Test Accuracy: 0.9507\n",
            "Epoch: 1\n",
            "Training Loss:0.0027,           Training Accuracy:0.9087\n",
            "Test Loss: 0.0012,           Test Accuracy: 0.9638\n",
            "Epoch: 2\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m'\u001b[39m,epoch)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Train loss\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_model(model3, optimizer, loss_function, train_loader)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Test loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m test_model(model3, loss_function, test_loader)\n",
            "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_function, train_loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m dataset_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Go over each batch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     14\u001b[0m   X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m   y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[0;32m~/miniconda3/envs/vesuvius/lib/python3.11/site-packages/torchvision/transforms/functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], F_pil\u001b[39m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Let's train\n",
        "loss_function = torch.nn.CrossEntropyLoss()     \n",
        "optimizer = torch.optim.SGD(model3.parameters(),lr=0.001)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print('Epoch:',epoch)\n",
        "  # Train loss\n",
        "  train_model(model3, optimizer, loss_function, train_loader)\n",
        "\n",
        "  # Test loss\n",
        "  test_model(model3, loss_function, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
