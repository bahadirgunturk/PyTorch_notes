{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AutoGrad**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "z8PXJtvpAB2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tensors form a computational graph*"
      ],
      "metadata": {
        "id": "pGUMwi0EANOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ymr8uJpGAwno"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A tensor does not require gradient calculation by default\n",
        "x = torch.tensor(1.0)\n",
        "print('x.requires_grad:',x.requires_grad)\n",
        "print('x:',x)\n",
        "\n",
        "# You can set gradient calculation to true\n",
        "w = torch.tensor(0.5, requires_grad=True) \n",
        "print('w:',w)\n",
        "\n",
        "# Some attributes of a tensor\n",
        "print('w.data:',w.data)\n",
        "print('w.grad:',w.grad)\n",
        "print('w.requires_grad:',w.requires_grad)\n",
        "print('w.grad_fn:',w.grad_fn)\n",
        "print('w.is_leaf:',w.is_leaf)"
      ],
      "metadata": {
        "id": "npaLXe8N12H3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea4e7e9-cb54-47c3-ac11-e212285c30db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.requires_grad: False\n",
            "x: tensor(1.)\n",
            "w: tensor(0.5000, requires_grad=True)\n",
            "w.data: tensor(0.5000)\n",
            "w.grad: None\n",
            "w.requires_grad: True\n",
            "w.grad_fn: None\n",
            "w.is_leaf: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a simple graph\n",
        "a = torch.tensor(2.0,requires_grad=True)\n",
        "b = torch.tensor(3.0,requires_grad=True)\n",
        "\n",
        "print('a.data:',a.data)\n",
        "print('a.grad:',a.grad)\n",
        "print('a.requires_grad:',a.requires_grad)\n",
        "print('a.grad_fn:',a.grad_fn)\n",
        "print('a.is_leaf:',a.is_leaf)\n",
        "\n",
        "print('')\n",
        "\n",
        "# A function of at least a tensor\n",
        "c = 2 * a + b\n",
        "print('c.data:',c.data)\n",
        "print('c.grad:',c.grad)   \n",
        "print('c.requires_grad:',c.requires_grad)\n",
        "print('c.grad_fn:',c.grad_fn)   # c is a function of some tensors, so it has fn\n",
        "print('c.is_leaf:',c.is_leaf)   # c is not a leaf! c.grad is not calculated.\n",
        "\n",
        "print('')\n",
        "\n",
        "# backward pass; gradients of all leafs are calculated automatically\n",
        "c.backward()\n",
        "print('a.data:',a.data)\n",
        "print('a.grad:',a.grad)\n",
        "print('a.requires_grad:',a.requires_grad)\n",
        "print('a.grad_fn:',a.grad_fn)\n",
        "print('a.is_leaf:',a.is_leaf)\n",
        "\n",
        "print('')\n",
        "\n",
        "print('b.data:',b.data)\n",
        "print('b.grad:',b.grad)\n",
        "print('b.requires_grad:',b.requires_grad)\n",
        "print('b.grad_fn:',b.grad_fn)\n",
        "print('b.is_leaf:',b.is_leaf)\n"
      ],
      "metadata": {
        "id": "eVSzhF1gOpyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d4d555-db1f-4d9e-9284-204d264f9621"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.data: tensor(2.)\n",
            "a.grad: None\n",
            "a.requires_grad: True\n",
            "a.grad_fn: None\n",
            "a.is_leaf: True\n",
            "\n",
            "c.data: tensor(7.)\n",
            "c.grad: None\n",
            "c.requires_grad: True\n",
            "c.grad_fn: <AddBackward0 object at 0x7fb5329936a0>\n",
            "c.is_leaf: False\n",
            "\n",
            "a.data: tensor(2.)\n",
            "a.grad: tensor(2.)\n",
            "a.requires_grad: True\n",
            "a.grad_fn: None\n",
            "a.is_leaf: True\n",
            "\n",
            "b.data: tensor(3.)\n",
            "b.grad: tensor(1.)\n",
            "b.requires_grad: True\n",
            "b.grad_fn: None\n",
            "b.is_leaf: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-66e207b14d85>:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  print('c.grad:',c.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A larger graph...\n",
        "a = torch.tensor(2.0,requires_grad=True)\n",
        "b = torch.tensor(3.0,requires_grad=False)\n",
        "\n",
        "c = a * b\n",
        "d = a + c\n",
        "\n",
        "print('a.data:',a.data)\n",
        "print('a.grad:',a.grad)\n",
        "print('a.requires_grad:',a.requires_grad)\n",
        "print('a.grad_fn:',a.grad_fn)\n",
        "print('a.is_leaf:',a.is_leaf)\n",
        "\n",
        "print('')\n",
        "print('c.data:',c.data)\n",
        "print('c.grad:',c.grad)   \n",
        "print('c.requires_grad:',c.requires_grad)\n",
        "print('c.grad_fn:',c.grad_fn)\n",
        "print('c.is_leaf:',c.is_leaf)\n",
        "\n",
        "print('')\n",
        "print('d.data:',d.data)\n",
        "print('d.grad:',d.grad)   \n",
        "print('d.requires_grad:',d.requires_grad)\n",
        "print('d.grad_fn:',d.grad_fn)\n",
        "print('d.is_leaf:',d.is_leaf)\n",
        "\n",
        "print('')\n",
        "print('After backward pass...')\n",
        "d.backward()  # grads for all tensors with requires_grad=True are calculated\n",
        "              #   except for grads for tensors that are not leaf.\n",
        "print('a.data:',a.data)\n",
        "print('a.grad:',a.grad)\n",
        "print('a.requires_grad:',a.requires_grad)\n",
        "print('a.grad_fn:',a.grad_fn)\n",
        "print('a.is_leaf:',a.is_leaf)\n",
        "\n",
        "print('')\n",
        "print('b.data:',b.data)\n",
        "print('b.grad:',b.grad)\n",
        "print('b.requires_grad:',b.requires_grad)\n",
        "print('b.grad_fn:',b.grad_fn)\n",
        "print('b.is_leaf:',b.is_leaf)\n",
        "\n",
        "print('')\n",
        "print('c.data:',c.data)\n",
        "print('c.grad:',c.grad)     # gradient is not stored since it is not a leaf node\n",
        "print('c.requires_grad:',c.requires_grad)\n",
        "print('c.grad_fn:',c.grad_fn)\n",
        "print('c.is_leaf:',c.is_leaf)\n"
      ],
      "metadata": {
        "id": "T1TxvweSRGXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e90b0f0-7259-44be-ec58-ce4b2f4c074b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.data: tensor(2.)\n",
            "a.grad: None\n",
            "a.requires_grad: True\n",
            "a.grad_fn: None\n",
            "a.is_leaf: True\n",
            "\n",
            "c.data: tensor(6.)\n",
            "c.grad: None\n",
            "c.requires_grad: True\n",
            "c.grad_fn: <MulBackward0 object at 0x7fb5329ac6d0>\n",
            "c.is_leaf: False\n",
            "\n",
            "d.data: tensor(8.)\n",
            "d.grad: None\n",
            "d.requires_grad: True\n",
            "d.grad_fn: <AddBackward0 object at 0x7fb5b81785b0>\n",
            "d.is_leaf: False\n",
            "\n",
            "After backward pass...\n",
            "a.data: tensor(2.)\n",
            "a.grad: tensor(4.)\n",
            "a.requires_grad: True\n",
            "a.grad_fn: None\n",
            "a.is_leaf: True\n",
            "\n",
            "b.data: tensor(3.)\n",
            "b.grad: None\n",
            "b.requires_grad: False\n",
            "b.grad_fn: None\n",
            "b.is_leaf: True\n",
            "\n",
            "c.data: tensor(6.)\n",
            "c.grad: None\n",
            "c.requires_grad: True\n",
            "c.grad_fn: <MulBackward0 object at 0x7fb5329945b0>\n",
            "c.is_leaf: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-9626f421c7bb>:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  print('c.grad:',c.grad)\n",
            "<ipython-input-18-9626f421c7bb>:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  print('d.grad:',d.grad)\n",
            "<ipython-input-18-9626f421c7bb>:47: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  print('c.grad:',c.grad)     # gradient is not stored since it is not a leaf node\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's solve for w in y = w * x, where y and x are given below\n",
        "y = torch.tensor(2.0)\n",
        "x = torch.tensor(1.0)\n",
        "\n",
        "w = torch.tensor(0.5, requires_grad=True)\n",
        "print('w before update:',w)\n",
        "\n",
        "# Try the following for more than 1 iteration\n",
        "for iter in range(1):\n",
        "  \n",
        "  # forward pass\n",
        "  y_hat = w * x\n",
        "  print('y_hat:',y_hat)  # this is a result of an operation, so there is grad_fn\n",
        "\n",
        "  # calculate loss\n",
        "  loss = (y_hat-y) **2\n",
        "  print('loss:',loss)    # this is a result of an operation, so there is grad_fn\n",
        "\n",
        "  # backward pass \n",
        "  #   - for all tensors with requires_grad, the gradients will be calculated\n",
        "  print('Gradient before back. pass:',w.grad)\n",
        "  loss.backward()                             # gradient of loss = -2*(y_hat-y)\n",
        "  print('Gradient after back. pass:',w.grad)\n",
        "\n",
        "  #w.grad.zero_()\n",
        "\n",
        "  #print(w)\n",
        "\n",
        "  # update w\n",
        "  #   If you do the below without torch.no_grad(), you will get an error:\n",
        "  #     TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
        "  #   This is because updating makes it an \"intermediate\" (non-leaf) tensor,\n",
        "  #     which makes it grad type None and requires_grad becomes False.\n",
        "  #     So, in torch.no_grad() context; thus autograd is disabled \n",
        "  #       https://www.youtube.com/watch?v=MswxJw-8PvE\n",
        "  with torch.no_grad():\n",
        "    w -= 0.1 * w.grad # in-place operation\n",
        "\n",
        "  print('w after update:',w)\n",
        "  print('Gradient after update:',w.grad)\n",
        "\n",
        "  # Zero the gradient - Gradients accumulate in tensors, so zero them.\n",
        "  w.grad.zero_()\n",
        "\n",
        "# Note the difference between torch.eval() and torch.no_grad()\n",
        "#   torch.eval(): all the layers will be in eval mode, including the  \n",
        "#                 batchnorm and dropout layers\n",
        "#   torch.no_grad(): deactivate the autograd engine\n"
      ],
      "metadata": {
        "id": "mDFlpRcS62zT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180dbc96-c1f8-44b7-be3c-5eab54ee0b5a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w before update: tensor(0.5000, requires_grad=True)\n",
            "y_hat: tensor(0.5000, grad_fn=<MulBackward0>)\n",
            "loss: tensor(2.2500, grad_fn=<PowBackward0>)\n",
            "Gradient before back. pass: None\n",
            "Gradient after back. pass: tensor(-3.)\n",
            "w after update: tensor(0.8000, requires_grad=True)\n",
            "Gradient after update: tensor(-3.)\n"
          ]
        }
      ]
    }
  ]
}