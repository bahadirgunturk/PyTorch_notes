{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RNN**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZZaU_ME_cV-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9yyfdg7A2Nc"
      },
      "outputs": [],
      "source": [
        "transform1 = transforms.Compose([transforms.Resize(28), # Change size if needed\n",
        "                                 transforms.ToTensor()])\n",
        "\n",
        "# download train dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./mnist_data/',\n",
        "                                           train=True,\n",
        "                                           transform=transform1,\n",
        "                                           download=True)\n",
        "\n",
        "# download test dataset\n",
        "test_dataset = torchvision.datasets.MNIST(root='./mnist_data/',\n",
        "                                           train=False,\n",
        "                                           transform=transform1,\n",
        "                                           download=True)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "batch_size = 32\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader =  torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "DywA2jEqCbza",
        "outputId": "3dddd6b6-a0bb-4a14-de0e-67c432786394"
      },
      "outputs": [],
      "source": [
        "# Check out the \n",
        "X, y = next(iter(train_loader))\n",
        "print(X.shape)\n",
        "\n",
        "img = transforms.ToPILImage()(X[0])\n",
        "plt.imshow(img,cmap='gray')\n",
        "plt.title(y[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSXTwAHwN4D_"
      },
      "outputs": [],
      "source": [
        "# Patchify\n",
        "# Split image into 7x7 patches, then vectorize. Image size is 28x28. Each patch is 4x4.\n",
        "# N,1,28,28 --> N, 49, 16\n",
        "\n",
        "def patchify(images,n_patches=7):\n",
        "  n, c, h, w = images.shape\n",
        "\n",
        "  patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2 )\n",
        "  patch_size = h // n_patches # 4\n",
        "\n",
        "  for idx, image in enumerate(images):\n",
        "    for i in range(n_patches):\n",
        "      for j in range(n_patches):\n",
        "        patch = image[:, i * patch_size: (i+1) * patch_size, j * patch_size: (j+1)*patch_size]\n",
        "        patches[idx, i * n_patches + j ] = patch.flatten()\n",
        "  \n",
        "  return patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkdkv3bdQU3L",
        "outputId": "cced719c-0e1c-45af-9ad5-887fc27a096d"
      },
      "outputs": [],
      "source": [
        "X, y = next(iter(train_loader))\n",
        "patches = patchify(X)\n",
        "print(patches.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "kRQ9teG2a6Ft",
        "outputId": "236e0703-3b6f-44fb-8258-ac5cdc3fc0d3"
      },
      "outputs": [],
      "source": [
        "# Positional encoding\n",
        "def get_positional_embeddings(sequence_length, d):\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
        "    return result\n",
        "\n",
        "pos_enc = get_positional_embeddings(50, 24)\n",
        "plt.imshow(pos_enc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFXuLbQMhwO3"
      },
      "outputs": [],
      "source": [
        "# Multi-Head Self Attention\n",
        "class MyMSA(nn.Module):\n",
        "  def __init__(self, d, n_heads=2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d = d  # embedding dimension\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    d_head = int(d / n_heads) # dimension of each head\n",
        "    self.d_head = d_head\n",
        "\n",
        "    # Use ModuleList to list modules to be appended\n",
        "    self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for i in range(self.n_heads)]) \n",
        "    self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for i in range(self.n_heads)]) \n",
        "    self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for i in range(self.n_heads)]) \n",
        "\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "  \n",
        "  def forward(self, sequences):\n",
        "    # sequences has shape (N, seq_length, token_dim)\n",
        "    # Map it to shape (N, seq_length, n_heads, token_dim / n_heads)\n",
        "    # And then, go back to (N, seq_length, token_dim) using concatenation\n",
        "    \n",
        "    result = []\n",
        "    for sequence in sequences:\n",
        "      seq_result = []\n",
        "      for head in range(self.n_heads):\n",
        "        q_mapping = self.q_mappings[head] # each is a nn.Linear\n",
        "        k_mapping = self.k_mappings[head]\n",
        "        v_mapping = self.v_mappings[head]\n",
        "\n",
        "        # Get part of a sequence\n",
        "        seq = sequence[:, head * self.d_head: (head+1) * self.d_head]\n",
        "\n",
        "        # And apply the mapping\n",
        "        q = q_mapping(seq)\n",
        "        k = k_mapping(seq)\n",
        "        v = v_mapping(seq)\n",
        "\n",
        "        # Attention\n",
        "        attention = self.softmax(q @ k.T / (self.d_head)**0.5 )\n",
        "        seq_result.append(attention @ v)\n",
        "      \n",
        "      result.append(torch.hstack(seq_result))\n",
        "    \n",
        "    return torch.cat([torch.unsqueeze(r,dim=0) for r in result])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9eLRcdLwWzk",
        "outputId": "a410ca58-c93f-4cf7-f354-972dcd346abd"
      },
      "outputs": [],
      "source": [
        "msa = MyMSA(8)\n",
        "X, y = next(iter(train_loader))\n",
        "out = patchify(X)\n",
        "print(out.shape)\n",
        "out2 = msa(out)\n",
        "print(out2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA9JRZubxMrI"
      },
      "outputs": [],
      "source": [
        "class MyViTBlock(nn.Module):\n",
        "  def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.hidden_d = hidden_d\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(hidden_d)\n",
        "    self.mhsa = MyMSA(hidden_d, n_heads)\n",
        "    self.norm2 = nn.LayerNorm(hidden_d)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = x + self.mhsa(self.norm1(x))\n",
        "    out = out + self.mlp(self.norm2(out))\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCUmlecyyzeO",
        "outputId": "407306df-d495-41fa-a0ea-dc9dbfc36a82"
      },
      "outputs": [],
      "source": [
        "model_vitblock = MyViTBlock(8,2)\n",
        "x = torch.randn(32,50,8)\n",
        "out = model_vitblock(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTphDowxEE_0"
      },
      "outputs": [],
      "source": [
        "# ViT\n",
        "\n",
        "class MyViT(nn.Module):\n",
        "  def __init__(self, chw=(1,28,28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.chw = chw\n",
        "    self.n_patches = n_patches\n",
        "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "    self.n_blocks = n_blocks\n",
        "    self.hidden_d = hidden_d\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    # 1) Linear mapping\n",
        "    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1]) \n",
        "    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "    # 2) Learnable classification token\n",
        "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "    # 3) Positional embedding\n",
        "    pos_enc = get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)\n",
        "    self.pos_embed = nn.Parameter(torch.tensor(pos_enc))\n",
        "    self.pos_embed.requires_grad = False\n",
        "\n",
        "    # 4) Transformer blocks\n",
        "    self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for i in range(n_blocks)])\n",
        "\n",
        "    # 5) Classifier\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(self.hidden_d, out_d),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    patches = patchify(X, self.n_patches)\n",
        "    tokens = self.linear_mapper(patches)\n",
        "\n",
        "    # Add the classification token for each image\n",
        "    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
        "\n",
        "    # Add positional embedding\n",
        "    pos_embed = self.pos_embed.repeat(tokens.size(0),1,1)\n",
        "    out = tokens + pos_embed\n",
        "\n",
        "    # Transformer blocks\n",
        "    for block in self.blocks:\n",
        "      out = block(out)\n",
        "    \n",
        "    # Get the classification token\n",
        "    out = out[:,0]\n",
        "\n",
        "    # Classifier mapping\n",
        "    out = self.mlp(out)\n",
        "\n",
        "    return out\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk1DWmttRfZ1",
        "outputId": "80458303-c693-43bb-90d1-0375dec397aa"
      },
      "outputs": [],
      "source": [
        "model = MyViT(\n",
        "    chw=(1,28,28),\n",
        "    n_patches=7)\n",
        "\n",
        "X, y = next(iter(train_loader))\n",
        "out = model(X)\n",
        "print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_-MFvfx01zI",
        "outputId": "7021010d-368a-4d53-87c0-3f2499683c65"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "N_EPOCHS = 5\n",
        "\n",
        "model.to(device)\n",
        "for epoch in range(N_EPOCHS):\n",
        "  total_loss = 0.0\n",
        "  for X,y in train_loader:\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    y_hat = model(X)\n",
        "\n",
        "    loss = criterion(y_hat, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "  \n",
        "  print(\"Epoch: \", epoch, \" Loss: \", total_loss)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
