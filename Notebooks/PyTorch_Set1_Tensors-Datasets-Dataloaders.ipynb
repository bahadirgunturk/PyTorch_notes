{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZtAhZidBdLrjJlfsikUlY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**PyTorch notes - Set 1**\n","\n","\n","---\n","\n","\n"],"metadata":{"id":"eiebfccKrVj7"}},{"cell_type":"markdown","source":["*Tensors*"],"metadata":{"id":"w-JpPHl_rY3K"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"k9N0ZMO2rR0f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667199238691,"user_tz":-180,"elapsed":4327,"user":{"displayName":"Bahadir Gunturk","userId":"10743444408372364025"}},"outputId":"d4dc235f-6291-43e0-e64a-698a0373d36a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1],\n","        [2, 3]]) torch.int64 <class 'torch.Tensor'>\n"]}],"source":["import torch\n","import numpy as np\n","\n","# Initialize a tensor from a list \n","data = [[0,1],[2,3]]                    # <class 'list'> \n","x = torch.tensor(data)                  # this is a tensor \n","print(x, x.dtype, type(x)) "]},{"cell_type":"code","source":["# ... as we initialize numpy array from a list\n","x_np = np.array(data)\n","print(x_np,x_np.dtype,type(x_np))"],"metadata":{"id":"L7vhSecWtM4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can convert a numpy array to tensor and vice versa\n","x = torch.from_numpy(x_np)\n","print(x, x.dtype, type(x))\n","\n","t = torch.ones((3,2),dtype=torch.float32)\n","print(t, t.dtype, type(t))\n","t_np = t.numpy() \n","print(t_np, t_np.dtype, type(t_np))"],"metadata":{"id":"u4Ko7aYquShT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can define the shape of a tensor\n","shape = (2)\n","x_rand = torch.rand(shape)                    # it is 1D vector\n","print(x_rand,x_rand.shape)\n","\n","x_rand_reshaped = x_rand.reshape(-1,1)        # reshape to a *2D* column vector\n","print(x_rand_reshaped,x_rand_reshaped.shape)\n","\n","shape = (2,1)\n","x_rand = torch.rand(shape)\n","print(x_rand,x_rand.shape)\n","\n","shape = (2,3,4)\n","x_rand = torch.rand(shape)\n","print(x_rand,x_rand.shape)"],"metadata":{"id":"98oduHBUwCY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change the shape of a tensor\n","x_rand = torch.rand([4000,28,28])\n","y1 = x_rand.view(x_rand.size(0),1,28,28)\n","print(y1.shape)\n","#print(y1.size())\n","y2 = x_rand.view(x_rand.size(0),-1)\n","print(y2.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-IdHRf2z4rb","executionInfo":{"status":"ok","timestamp":1667199294450,"user_tz":-180,"elapsed":260,"user":{"displayName":"Bahadir Gunturk","userId":"10743444408372364025"}},"outputId":"90a46e50-a0d1-4e88-bee2-bea627411f50"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4000, 1, 28, 28])\n","torch.Size([4000, 1, 28, 28])\n","torch.Size([4000, 784])\n"]}]},{"cell_type":"code","source":["# Check out the dimensions of the following\n","\n","x_seq = torch.tensor( 1.0 )\n","print('torch.tensor( 1.0 ) >>',x_seq.shape) # 0 dimension\n","\n","#x_seq = torch.tensor( 1.0, 1.0 ) #ERROR! tensor takes 1 arg.\n","#print(x_seq.shape)\n","\n","x_seq = torch.tensor( [1.0] )\n","print('torch.tensor([ 1.0 ]) >>', x_seq.shape) # 1 dimension, size 1\n","\n","x_seq = torch.tensor( [1.0, 1.0] )\n","print('torch.tensor([ 1.0, 1.0 ]) >>',x_seq.shape) # 1 dimension, size 2\n","\n","x_seq = torch.tensor([ [1.0] ]) # putting a second bracket adds another dim.\n","print('torch.tensor([ [1.0] ]) >>', x_seq.shape) # 2 dimension, each size 1\n","\n","x_seq = torch.tensor([ [1.0], [1.0] ])\n","print('torch.tensor([ [1.0], [1.0] ]) >>', x_seq.shape)\n","\n","x_seq = torch.tensor([ [1.0, 1.0] ])\n","print('torch.tensor([ [1.0, 1.0] ]) >>', x_seq.shape)\n","\n","x_seq = torch.tensor( [ [ [1.0,1.0,1.0,1.0,1.0], [1.0,1.0,1.0,1.0,1.0] ] ] )\n","print(x_seq.shape)\n","\n","x_seq = torch.tensor([ \n","    [ [1.0,1.0,1.0,1.0,1.0], [1.0,1.0,1.0,1.0,1.0] ] \n","])\n","print(x_seq.shape)\n","\n","x_seq = torch.tensor([ \n","    [ [1.0,1.0,1.0,1.0,1.0], [1.0,1.0,1.0,1.0,1.0] ],\n","    [ [1.0,1.0,1.0,1.0,1.0], [1.0,1.0,1.0,1.0,1.0] ] \n","])\n","print(x_seq.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4zCHAIk07Nr","executionInfo":{"status":"ok","timestamp":1667200753732,"user_tz":-180,"elapsed":296,"user":{"displayName":"Bahadir Gunturk","userId":"10743444408372364025"}},"outputId":"a0ed8460-661a-4efd-b0cb-3fb0ee5a9fd3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.tensor( 1.0 ) >> torch.Size([])\n","torch.tensor([ 1.0 ]) >> torch.Size([1])\n","torch.tensor([ 1.0, 1.0 ]) >> torch.Size([2])\n","torch.tensor([ [1.0] ]) >> torch.Size([1, 1])\n","torch.tensor([ [1.0], [1.0] ]) >> torch.Size([2, 1])\n","torch.tensor([ [1.0, 1.0] ]) >> torch.Size([1, 2])\n","torch.Size([1, 2, 5])\n","torch.Size([1, 2, 5])\n","torch.Size([2, 2, 5])\n"]}]},{"cell_type":"code","source":["# You can slice a tensor\n","x_rand = torch.rand((2,3))\n","print(x_rand)\n","print(f\"First row: {x_rand[0]}\")\n","print(f\"First row: {x_rand[0,:]}\")\n","print(f\"First column: {x_rand[:, 0]}\")\n","print(f\"Last column: {x_rand[:, -1]}\")\n","print(f\"A slice: {x_rand[0:2, 0:2]}\")\n","print(f\"A detached element: {x_rand[0,0].item()}\")"],"metadata":{"id":"3q4Uaqvyw8cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Slice at higher dimensions\n","x_seq = torch.randn(1,3,5)\n","print(x_seq.shape)\n","\n","print(x_seq)\n","\n","# At the first time instant, the feature vector is\n","print(x_seq[0,0,:])\n","\n","# At the second time instant, the feature vector is\n","print(x_seq[0,1,:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fau7FpQo4EdU","executionInfo":{"status":"ok","timestamp":1667200321625,"user_tz":-180,"elapsed":267,"user":{"displayName":"Bahadir Gunturk","userId":"10743444408372364025"}},"outputId":"be193351-e620-4751-f450-42577c4f0211"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 5])\n","tensor([[[-0.7981,  1.1100,  2.1945,  0.8437,  0.6368],\n","         [ 0.1701, -0.5426, -0.0547, -0.8019, -0.7363],\n","         [ 0.0026, -0.4802, -2.6052,  0.6449,  0.8568]]])\n","tensor([-0.7981,  1.1100,  2.1945,  0.8437,  0.6368])\n","tensor([ 0.1701, -0.5426, -0.0547, -0.8019, -0.7363])\n"]}]},{"cell_type":"code","source":["# Concatenate tensors\n","t = torch.ones((1,2))\n","t1 = torch.cat([t, t, t],dim=1)   # horizontally\n","print(t1)\n","t2 = torch.cat([t, t, t],dim=0)   # vertically\n","print(t2)"],"metadata":{"id":"cGyntdPiy1h0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Arithmetic operations\n","t1 = torch.ones((1,2))\n","\n","# - matrix multiplication\n","y1 = t1 @ t1.T\n","print(y1)\n","y2 = t1.matmul(t1.T)\n","print(y2)\n","\n","# - element-wise multiplication\n","z1 = t1 * t1\n","print(z1)\n","z2 = t1.mul(t1)\n","print(z2)"],"metadata":{"id":"4xEkjNS9zoVJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Tensors as a form of computational graph* "],"metadata":{"id":"SDNkDbCI1iU2"}},{"cell_type":"code","source":["# A tensor does not require gradient calculation by default\n","x = torch.tensor(1.0)\n","print(x.requires_grad)\n","print(x)\n","\n","# You can set gradient calculation to true\n","w = torch.tensor(0.5, requires_grad=True) \n","print(w)\n","\n","# Some attributes of a tensor\n","print('w.data:',w.data)\n","print('w.grad:',w.grad)\n","print('w.requires_grad:',w.requires_grad)\n","print('w.grad_fn:',w.grad_fn)\n","print('w.is_leaf:',w.is_leaf)"],"metadata":{"id":"npaLXe8N12H3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's create a simple graph\n","a = torch.tensor(2.0,requires_grad=True)\n","b = torch.tensor(3.0,requires_grad=True)\n","\n","print('a.data:',a.data)\n","print('a.grad:',a.grad)\n","print('a.requires_grad:',a.requires_grad)\n","print('a.grad_fn:',a.grad_fn)\n","print('a.is_leaf:',a.is_leaf)\n","\n","print('')\n","\n","# A function of at least a tensor\n","c = 2 * a\n","print('c.data:',c.data)\n","print('c.grad:',c.grad)   # grad is no longer held because it is not a leaf node\n","print('c.requires_grad:',c.requires_grad)\n","print('c.grad_fn:',c.grad_fn)\n","print('c.is_leaf:',c.is_leaf)\n","\n","print('')\n","\n","# backward pass and then check out the leaf tensor\n","c.backward()\n","print('a.data:',a.data)\n","print('a.grad:',a.grad)\n","print('a.requires_grad:',a.requires_grad)\n","print('a.grad_fn:',a.grad_fn)\n","print('a.is_leaf:',a.is_leaf)\n"],"metadata":{"id":"eVSzhF1gOpyJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A larger graph...\n","a = torch.tensor(2.0,requires_grad=True)\n","b = torch.tensor(3.0,requires_grad=False)\n","\n","c = a * b\n","d = a + c\n","\n","print('a.data:',a.data)\n","print('a.grad:',a.grad)\n","print('a.requires_grad:',a.requires_grad)\n","print('a.grad_fn:',a.grad_fn)\n","print('a.is_leaf:',a.is_leaf)\n","\n","print('')\n","print('c.data:',c.data)\n","print('c.grad:',c.grad)   \n","print('c.requires_grad:',c.requires_grad)\n","print('c.grad_fn:',c.grad_fn)\n","print('c.is_leaf:',c.is_leaf)\n","\n","print('')\n","print('d.data:',d.data)\n","print('d.grad:',d.grad)   \n","print('d.requires_grad:',d.requires_grad)\n","print('d.grad_fn:',d.grad_fn)\n","print('d.is_leaf:',d.is_leaf)\n","\n","print('')\n","print('After backward pass...')\n","d.backward()  # grads for all tensors with requires_grad=True are calculated\n","              #   except for grads for tensors that are not leaf.\n","print('a.data:',a.data)\n","print('a.grad:',a.grad)\n","print('a.requires_grad:',a.requires_grad)\n","print('a.grad_fn:',a.grad_fn)\n","print('a.is_leaf:',a.is_leaf)\n","\n","print('')\n","print('b.data:',b.data)\n","print('b.grad:',b.grad)\n","print('b.requires_grad:',b.requires_grad)\n","print('b.grad_fn:',b.grad_fn)\n","print('b.is_leaf:',b.is_leaf)\n","\n","print('')\n","print('c.data:',c.data)\n","print('c.grad:',c.grad)     # gradient is not stored since it is not a leaf node\n","print('c.requires_grad:',c.requires_grad)\n","print('c.grad_fn:',c.grad_fn)\n","print('c.is_leaf:',c.is_leaf)\n"],"metadata":{"id":"T1TxvweSRGXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's solve for w in y = w * x, where y and x are given below\n","y = torch.tensor(2.0)\n","x = torch.tensor(1.0)\n","\n","w = torch.tensor(0.5, requires_grad=True)\n","print('w before update:',w)\n","\n","# Try the following for more than 1 iteration\n","for iter in range(1):\n","  # forward pass\n","  y_hat = w * x\n","  print('y_hat:',y_hat)  # this is a result of an operation, so there is grad_fn\n","\n","  # calculate loss\n","  loss = (y_hat-y) **2\n","  print('loss:',loss)    # this is a result of an operation, so there is grad_fn\n","\n","  # backward pass \n","  #   - for all tensors with requires_grad, the gradients will be calculated\n","  print('Gradient before back. pass:',w.grad)\n","  loss.backward()                             # gradient of loss = -2*(y_hat-y)\n","  print('Gradient after back. pass:',w.grad)\n","\n","  #w.grad.zero_()\n","\n","  #print(w)\n","\n","  # update w\n","  #   If you do the below without torch.no_grad(), you will get an error:\n","  #     TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n","  #   This is because updating makes it an \"intermediate\" (non-leaf) tensor,\n","  #     which makes it grad type None and requires_grad becomes False.\n","  #     So, in torch.no_grad() context; thus autograd is disabled \n","  #       https://www.youtube.com/watch?v=MswxJw-8PvE\n","  with torch.no_grad():\n","    w -= 0.1 * w.grad # in-place operation\n","\n","  print('w after update:',w)\n","  print('Gradient after update:',w.grad)\n","\n","  # Zero the gradient - Gradients accumulate in tensors, so zero them.\n","  w.grad.zero_()\n","\n","# Note the difference between torch.eval() and torch.no_grad()\n","#   torch.eval(): all the layers will be in eval mode, including the  \n","#                 batchnorm and dropout layers\n","#   torch.no_grad(): deactivate the autograd engine\n"],"metadata":{"id":"mDFlpRcS62zT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Datasets from PyTorch*"],"metadata":{"id":"ziXC_usVcJk_"}},{"cell_type":"code","source":["# There are prepared datasets for vision, text and audio\n","from torchvision import datasets\n","from torchvision import transforms\n","from matplotlib import pyplot as plt\n","\n","# Create a transform to be applied to each sample\n","#   .ToTensor() is critical; without it, dataloader will give error!\n","transform1 = transforms.Compose([\n","    transforms.ToTensor(),  # converts PIL image or numpy array (HxWxC) into a \n","                            #   torch.FloatTensor (CxHxW), scales it to [0,1] \n","    transforms.Normalize(mean=[0.5],std=[0.5]) # for 1 channel\n","                            #   [0.5,0.5,0.5] for 3 channels\n","                            # Conversion from RGB to gray could also be useful\n","                            # Transforms can also be added to the dataloader\n","    ])\n","\n","# Download the dataset and apply the transform for each sample\n","train_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform = transform1\n","    )\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform = transform1\n","    )\n","\n","print(train_data.data.shape) \n","print(train_data.targets.shape)\n","\n","sample_id = 0\n","image = train_data.data[sample_id]\n","label = train_data.targets[sample_id]\n","\n","print(type(image),image.shape, image[20,20]) \n","# print(type(label),label.shape)\n","\n","plt.imshow(image, cmap=\"gray\") # .squeeze() if img.shape is (1,28,28)\n","plt.title(label)\n"],"metadata":{"id":"kOxxf8J9cOjg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Dataloader*"],"metadata":{"id":"wyeD-IdYGz7Q"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader #creates an iterable around a dataset\n","\n","batch_size = 64\n","\n","train_dataloader = DataLoader(train_data,batch_size,shuffle=True)\n","test_dataloader = DataLoader(test_data,batch_size,shuffle=False)\n","\n","# Working with a dataloader\n","for X,y in train_dataloader:\n","  print(X.shape) #batchsize, number of channels, height, width\n","  print(y.shape) #batchsize\n","  break # one run of the dataloader\n","\n","sample_id = 0\n","image = X[sample_id,:,:,:]\n","label = y[sample_id]\n","\n","#print(image.shape) # torch.Size([1, 28, 28])\n","\n","plt.imshow(image.squeeze(), cmap=\"gray\") # we squeeze because shape is (1,28,28)\n","plt.title(label)\n","\n","# visualize the batch of the dataset\n","import torchvision\n","# X is of size (b,c,h,w)\n","grid = torchvision.utils.make_grid(X,nrow=10) # grid is a tensor of size (c,h,w)\n","                                              # nrow: number of imgs in each row\n","transform_tensor_to_PIL = transforms.ToPILImage() \n","grid_img = transform_tensor_to_PIL(grid)      # (h,w,c)\n","plt.imshow(grid_img)\n"],"metadata":{"id":"Wr-lGvr1GySa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Datasets from numpy arrays*"],"metadata":{"id":"ShBNg-LZxYWt"}},{"cell_type":"code","source":["# Let's download a dataset from sklearn\n","from sklearn.datasets import load_digits\n","digits = load_digits()\n","\n","#print(digits.images.shape)  # (1797, 8, 8)\n","#print(type(digits.images))  # <class 'numpy.ndarray'>\n","\n","# Let's do train - test split\n","X = digits.images\n","y = digits.target\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X,y,\n","                                                    test_size=0.20,\n","                                                    shuffle=True)\n","\n","# First, transform numpy to tensor\n","# X_train is of size (batch,h,w). Convert it to (b,c,h,w)\n","X_train = torch.tensor(X_train).unsqueeze(1)\n","X_test = torch.tensor(X_test).unsqueeze(1)\n","\n","y_train = torch.tensor(y_train)\n","y_test = torch.tensor(y_test)\n"],"metadata":{"id":"U_aDwGI8Fcok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Create a custom dataset from tensors*"],"metadata":{"id":"inGyydScBPD_"}},{"cell_type":"code","source":["# We do this by using TensorDataset\n","from torch.utils.data import TensorDatasethange SGD to Adam, we simply change optim.SGD to optim.Adam, also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initia\n","\n","# Second, convert the tensors to dataset\n","train_data = TensorDataset(X_train,y_train)\n","test_data = TensorDataset(X_test,y_test)\n","\n","# Create the dataloaders\n","train_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)\n","\n","# Working with a dataloader\n","for X,y in train_dataloader:\n","  print(X.shape) #batchsize, number of channels, height, width\n","  print(y.shape) #batchsize\n","  break # one run of the dataloader\n","\n","# Alternatively, ...\n","X, y = next(iter(train_dataloader))\n","print(X.shape)\n","print(y.shape)\n","\n","# Visualize\n","grid = torchvision.utils.make_grid(X,nrow=16) # grid is a tensor of size (c,h,w)\n","                                              # nrow: number of imgs in each row\n","transform_tensor_to_PIL = transforms.ToPILImage() \n","grid_img = transform_tensor_to_PIL(grid)      # (h,w,c)\n","plt.imshow(grid_img)\n"],"metadata":{"id":"rN1V9rvEfk2B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Create a custom dataset from image files*"],"metadata":{"id":"g2bFvJ12kOa_"}},{"cell_type":"code","source":["# We do this by writing a Dataset class \n","import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n","from torchvision.transforms import transforms\n","from skimage import io\n","\n","# we need a folder with images\n","# we need a csv or txt annotation file with image filenames and labels, e.g.:\n","#   img1.jpg, 0\n","#   img2.jpg, 0\n","#   img3.jpg, 1\n","#   ...\n","\n","class CustomImageDataset(Dataset):\n","  def __init__(self,annotation_file, img_dir, transform=None, target_transform=None):\n","    self.images_labels_df = pd.read_csv(annotation_file, \n","                                        header=None, \n","                                        names=[\"Image name\", \"Label\"])\n","    self.img_dir = img_dir\n","    self.transform = transform\n","    self.target_transform = target_transform\n","  \n","  def __len__(self):\n","    return len(self.images_labels_df)\n","  \n","  def __getitem__(self,idx):\n","    #iloc indexed location, 0 is filename, 1 is target label\n","    img_path = os.path.join(self.img_dir, self.images_labels_df.iloc[idx,0]) \n","    image = read_image(img_path) # this produces tensor of size c,h,w\n","    #image = io.imread(img_path) # if used, transform w/ transforms.ToTensor()\n","\n","    # do not forget to convert the label to torch tensor as well\n","    label = torch.tensor((self.images_labels_df.iloc[idx,1])) \n","    \n","    if self.transform:\n","      image = self.transform(image)\n","    \n","    if self.target_transform:\n","      label = self.target_transform(label)\n","\n","    return image, label\n","\n","\n","# Let's now check it out\n","\n","# Directory and annotation file\n","img_dir = './sample_data/images/'\n","annotation_file = './sample_data/images/labels.txt'\n","\n","# Create the custom dataset\n","custom_dataset = CustomImageDataset(annotation_file=annotation_file, \n","                                    img_dir=img_dir, \n","                                    transform=None)  \n","\n","# Create the dataloader\n","dataloader = DataLoader(dataset=custom_dataset,batch_size=1)\n","\n","# Check it out\n","X, y = next(iter(dataloader))\n","print(X.shape)\n","print(y.shape)\n","\n","\n"],"metadata":{"id":"ly2eUU8JkSpY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666011210566,"user_tz":-180,"elapsed":415,"user":{"displayName":"Bahadir Gunturk","userId":"10743444408372364025"}},"outputId":"afb81adf-f077-4661-feb2-2b0967b4fdf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 2048, 2048])\n","torch.Size([1])\n"]}]}]}