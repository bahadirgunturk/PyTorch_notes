{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usG3F0MmGhCM"
      },
      "source": [
        "**RNN**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI2ZXnToPYQD"
      },
      "source": [
        "*Let's first create an RNN of with input feature vector size 5, hidden vector (unit) size 2*\n",
        "\n",
        "*The output of RNN is hidden unit values at all time steps. It is of size (batch_size, sequence_length, hidden_size x num_directions) if batch_first=True; otherwise, (sequence_length, batch_size, num_directions * hidden_size)*\n",
        "\n",
        "*num_directions is 2 for bidirectional RNN, where the data is input in the reverse order to a secondary network.*  \n",
        "\n",
        "Check out the following\n",
        "\n",
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9-E20xKGll0",
        "outputId": "d7c63ab0-6c7a-48d0-bb1e-e06a7e6d3e3b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# create an RNN layer \n",
        "#   input_size is the feature length; hidden_size is the number of hidden units\n",
        "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=2, batch_first=True)\n",
        "\n",
        "print(rnn_layer)\n",
        "\n",
        "print('# Print the initial input-to-hidden weights and biases @ the 1st layer')\n",
        "print(rnn_layer.weight_ih_l0)\n",
        "print(rnn_layer.bias_ih_l0)\n",
        "print('# Print the initial hidden-to-hidden weights and biases @ the 1st layer')\n",
        "print(rnn_layer.weight_hh_l0)\n",
        "print(rnn_layer.bias_hh_l0)\n",
        "\n",
        "# For the second layer, rnn_layer.weight_ih_l1, ...\n",
        "\n",
        "# Note that there is no separate output; the hidden state is used as the output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiZD0DSAImpq"
      },
      "source": [
        "*Let's create an input sequence*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GDLQU7NItTE",
        "outputId": "54069e69-4b7e-4507-a97f-874cca0b6bc5"
      },
      "outputs": [],
      "source": [
        "# When batch_first = True, the expected shape of input is\n",
        "#   (batch_size, sequence_length, input_size)\n",
        "# Let's create a sequence of size (1,3,5)\n",
        "\n",
        "x_seq = torch.randn(1,3,5)\n",
        "print(x_seq)\n",
        "\n",
        "# At the first time instant, the feature vector is\n",
        "print(x_seq[0,0,:])\n",
        "\n",
        "# At the second time instant, the feature vector is\n",
        "print(x_seq[0,1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huoH1ar0S3Pm"
      },
      "source": [
        "*Let's get the output*\n",
        "\n",
        "RNN has two outputs:\n",
        "\n",
        "**out**: the output from all time steps. (batch_size, sequence_length, num_directions x hidden_size). If there are multiple layers, this is the output of last layer at all time steps.\n",
        "\n",
        "**h_n**: hidden unit values from the last step. (num_layers x num_directions, batch_size, hidden_size). If there are multiple layers, it is the hidden states of all layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtldx2UaS_YG",
        "outputId": "10158ba7-bd11-4b4e-a6b6-6f4a289602ff"
      },
      "outputs": [],
      "source": [
        "out, h_n = rnn_layer(x_seq)\n",
        "print(out)\n",
        "print(h_n)\n",
        "# Above, check out the last step value of out and h_n. \n",
        "#   Try it when batch_first=False and with different num_layers\n",
        "print(out.shape)\n",
        "print(h_n.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoj6DZf1dVnE"
      },
      "source": [
        "*Now, let's do sentiment analysis with real data*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll8B92QFdgv3",
        "outputId": "0403e17e-113e-40e8-b3ab-fae61046668f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "PATH = '/content/drive/MyDrive/Colab Notebooks/sample_data/'\n",
        "train_dataset = pd.read_csv(PATH+\"imdb_train.csv\")\n",
        "valid_dataset = pd.read_csv(PATH+\"imdb_valid.csv\")\n",
        "test_dataset = pd.read_csv(PATH+\"imdb_test.csv\")\n",
        "\n",
        "#print(dir(train_dataset))\n",
        "print(train_dataset.head())\n",
        "print(train_dataset.label.unique()) # there are two labels: 0, 1\n",
        "\n",
        "# Print a specific review and its review class\n",
        "print('First sentence:',train_dataset.text[0])\n",
        "print('First label:',train_dataset.label[0])\n",
        "\n",
        "print('Training test size:',len(train_dataset.label))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDRSO4wqQkoJ"
      },
      "source": [
        "*Next, we have to build a dictionary; that is, assign a codeword for each unique word [token]. One-hot-encoding is a popular choice. However, the number of unique words could be too much. So, it is better to limit it. We can take, for example, the most common n words. Then, replace the words that are not in the dictionary as \"unknown\".*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eiUgtP9R2Uj",
        "outputId": "6260787f-ae79-4bec-d88f-ac2ba0034adb"
      },
      "outputs": [],
      "source": [
        "import re # Regular expressions\n",
        "\n",
        "def tokenizer(text):\n",
        "  # returns a list of unique words, while removing special characters, etc.\n",
        "  text = re.sub('<[^>]*>', '', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "  text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-','')\n",
        "  tokenized = text.split()\n",
        "  return tokenized\n",
        "\n",
        "print(tokenizer('Hi! Selam...!  Are you still working? Good luck:)'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsvMbiZ98TQh",
        "outputId": "ef521b8c-864f-4bd8-97ae-cc60c1820cd2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Now, find the counts of each token in the train dataset\n",
        "token_counts = Counter() # a dictionary of words (as key) with counts (as value)\n",
        "\n",
        "for i in range(len(train_dataset.label)):\n",
        "  text = train_dataset.text[i]\n",
        "  label = train_dataset.label[i]\n",
        "  tokens = tokenizer(text)\n",
        "  token_counts.update(tokens) \n",
        "\n",
        "print('Dictionary size:', len(token_counts))\n",
        "\n",
        "print(token_counts) # ordered based on appearance\n",
        "\n",
        "print('The number of appearance for \"hi\" is:',token_counts['hi'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59fBDq53YuM_"
      },
      "source": [
        "*Now, let's create a vocabulary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw5vcaSvY7zL",
        "outputId": "9787acaf-3e39-4ee3-cc25-0642dbf9e97e"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "#from torchtext import data\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "\n",
        "MAX_NO_OF_WORDS = 25000\n",
        "\n",
        "# the below is necessary only when token_counts is not ordered\n",
        "tokens_sorted_by_freq = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Take the first MAX_NO_OF_WORDS\n",
        "tokens_sorted_by_freq = tokens_sorted_by_freq[0:MAX_NO_OF_WORDS]\n",
        "\n",
        "# Now build a vocabulary, first get an ordered dictionary of tokens\n",
        "tokens_ordered_dict = OrderedDict(tokens_sorted_by_freq)\n",
        "print(tokens_ordered_dict)\n",
        "\n",
        "# And then, get the vocabulary\n",
        "vocabulary = vocab(tokens_ordered_dict) # maps tokens to indices\n",
        "print(vocabulary['the'])  \n",
        "\n",
        "\n",
        "# Add two new tokens to the vocabulary\n",
        "vocabulary.insert_token(\"<pad>\", 0) # this is for padding to adjust seq. length\n",
        "vocabulary.insert_token(\"<unk>\", 1) # this is for unknown words\n",
        "vocabulary.set_default_index(1)\n",
        "\n",
        "print(vocabulary['<pad>'])\n",
        "print(vocabulary['the'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRmr8i8E6dei"
      },
      "source": [
        "*Now, let's create a custom dataset. Note that train_dataset is not a PyTorch dataset, we need to define __ init__, __ len__ and __ getitem__ methods. Also, we need to transform the text according to vocabulary we have built.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5bH4eeY7PF8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self,text_dataset,vocab):\n",
        "    # text_dataset is a pandas dataframe, with .text and .label attributes\n",
        "    self.text_dataset = text_dataset \n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.text_dataset.label)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    a_text = self.text_dataset.text[idx] \n",
        "\n",
        "    text = self.transform_text(a_text)\n",
        "    label = torch.tensor(self.text_dataset.label[idx])\n",
        "    textlength = torch.tensor(len(text.detach()))\n",
        "\n",
        "    return text, label, textlength\n",
        "\n",
        "  def transform_text(self,text):\n",
        "    # Define a function to transform text to vocab indices\n",
        "    text_pipeline = lambda x: [self.vocab[token] for token in tokenizer(x)]\n",
        "    \n",
        "    # the following should be a list of indices\n",
        "    transformed_text = text_pipeline(text)\n",
        "    transformed_text = torch.tensor(transformed_text,dtype=torch.int64)\n",
        "\n",
        "    return transformed_text\n",
        "\n",
        "train_textdataset = TextDataset(train_dataset,vocabulary)\n",
        "valid_textdataset = TextDataset(valid_dataset,vocabulary)\n",
        "test_textdataset = TextDataset(test_dataset,vocabulary)\n",
        "\n",
        "# Create a dataloader with batch_size = 1\n",
        "dataloader = DataLoader(train_textdataset, batch_size=1,shuffle=True)\n",
        "\n",
        "# Check it out\n",
        "text_batch, label_batch, textlength_batch = next(iter(dataloader))\n",
        "print('text batch:',text_batch)\n",
        "print('label batch:',label_batch)\n",
        "print('text length batch',textlength_batch)\n",
        "\n",
        "# The above is going to work for batch_size=1; however, for other batch sizes, \n",
        "#   it will give error because it expects each item in batch size to be equal.\n",
        "#   Therefore, we need to define a collate (combine) function\n",
        "\n",
        "def collate_batch(batch):\n",
        "  text_list, label_list,length_list = [], [], []\n",
        "  for text, label, length in batch:\n",
        "    text_list.append(text)\n",
        "    label_list.append(label)\n",
        "    length_list.append(length)\n",
        "  \n",
        "  # Pad text to make sure all samples in the batch have same size\n",
        "  padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True) \n",
        "\n",
        "  # convert the label_list and length_list to tensor\n",
        "  label_list = torch.tensor(label_list,dtype=torch.float32)\n",
        "  length_list = torch.tensor(length_list,dtype=torch.float32)\n",
        "\n",
        "  return padded_text_list, label_list, length_list\n",
        "\n",
        "\n",
        "# Now, create a dataloader that can handle batches\n",
        "dataloader2 = DataLoader(train_textdataset, \n",
        "                         batch_size=2,\n",
        "                         shuffle=True,\n",
        "                         collate_fn=collate_batch)\n",
        "\n",
        "# Check it out\n",
        "text_batch, label_batch, textlength_batch = next(iter(dataloader2))\n",
        "print(text_batch)\n",
        "print(label_batch)\n",
        "print(textlength_batch)\n",
        "print(text_batch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh7cZ1XagAm-"
      },
      "source": [
        "*Let's create dataloaders for train, validation and test datasets*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8gbRCSsgJLR"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_dl = DataLoader(train_textdataset, \n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=True, \n",
        "                      collate_fn=collate_batch)\n",
        "valid_dl = DataLoader(valid_textdataset, \n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=False, \n",
        "                      collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_textdataset, \n",
        "                     batch_size=batch_size,\n",
        "                     shuffle=False, \n",
        "                     collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "# Check it out\n",
        "text_batch, label_batch, length_batch = next(iter(train_dl))\n",
        "print(text_batch.shape)\n",
        "print(label_batch.shape)\n",
        "print(length_batch.shape)\n",
        "print(text_batch)\n",
        "print(label_batch)\n",
        "print(length_batch)\n",
        "print(text_batch[0].dtype)\n",
        "print(label_batch[0].dtype)\n",
        "print(length_batch[0].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPwRx1UyhcPk"
      },
      "source": [
        "*We will do **feature embedding,** which is a dimensionality reduction approach for word vectors.*\n",
        "\n",
        "*At the moment, words in a text are integer numbers. One may do one-hot encoding to convert these integers to vectors of ones and zeros. However, the dimensionality would be very large.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-8un4oxifrF"
      },
      "outputs": [],
      "source": [
        "# An example of embedding, which is a linear mapping from indices to vectors\n",
        "#   The weights are initialized randomly. (Run this cell multiple times, and see)\n",
        "#   If part of a model, the weights are optimized through training\n",
        "embedding = nn.Embedding(num_embeddings=10,embedding_dim=2,padding_idx=0)\n",
        "# In the above, num_embeddings will be vocabulary size,\n",
        "#   embedding_dim is what we choose\n",
        "#   padding_idx is 0; for these gradient will not be calculated...\n",
        "\n",
        "# a sample input\n",
        "text_input = torch.LongTensor([[1,2,0,0],[5,4,2,1],[9,1,1,0]])\n",
        "print(text_input.shape)\n",
        "out = embedding(text_input)\n",
        "print(out)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWXPGeor6niq"
      },
      "source": [
        "*An LSTM model with feature embedding*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkqiRv9f6tMH"
      },
      "outputs": [],
      "source": [
        "class model_LSTM(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim,hidden_size,fc_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,\n",
        "                                  embed_dim,\n",
        "                                  padding_idx=0)\n",
        "    self.lstm = nn.LSTM(embed_dim,\n",
        "                        hidden_size,\n",
        "                        batch_first = True)\n",
        "    self.fc1 = nn.Linear(hidden_size,fc_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    # We have to sentiment outcomes 0 (negative) or 1 (positive).\n",
        "    #   So, use a single output unit, and then apply nn.Sigmoid()\n",
        "    #   Then, for the cost function, use BCEloss \n",
        "    self.fc2 = nn.Linear(fc_size,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,text,textlength):\n",
        "\n",
        "    # textlength is a tensor list\n",
        "    textlength = [x.item() for x in textlength]\n",
        "    #textlength = torch.stack(textlength)\n",
        "\n",
        "    out = self.embedding(text)\n",
        "\n",
        "\n",
        "    # Packing provides some computational efficiency\n",
        "    #   Check out: https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "    out = nn.utils.rnn.pack_padded_sequence(out, \n",
        "            textlength,\n",
        "            enforce_sorted=False, \n",
        "            batch_first=True)\n",
        "    '''\n",
        "    a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
        "    b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
        "    >>>>\n",
        "    tensor([[ 1,  2,  3],\n",
        "        [ 3,  4,  0]])\n",
        "    torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2])\n",
        "    >>>>PackedSequence(data=tensor([ 1,  3,  2,  4,  3]), batch_sizes=tensor([ 2,  2,  1]))\n",
        "    '''\n",
        "\n",
        "    _, (hidden,cell) = self.lstm(out)\n",
        "    out = hidden[-1,:,:]\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# parameters\n",
        "vocab_size = len(vocabulary)\n",
        "embed_dim = 20\n",
        "hidden_size = 64\n",
        "fc_size = 32\n",
        "model = model_LSTM(vocab_size,embed_dim,hidden_size,fc_size)\n",
        "print(model)\n",
        "\n",
        "\n",
        "# Check out the model with a sample input\n",
        "text_batch, label_batch, length_batch = next(iter(train_dl))\n",
        "# take the first two elements of the batch\n",
        "text_batch = text_batch[0:2,:]\n",
        "label_batch = label_batch[0:2]\n",
        "length_batch = length_batch[0:2]\n",
        "\n",
        "print(text_batch.shape)\n",
        "\n",
        "print(text_batch)\n",
        "print(label_batch)\n",
        "print(length_batch)\n",
        "\n",
        "\n",
        "model(text_batch,length_batch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcvFZf4P-c3-"
      },
      "source": [
        "*Now, let's define a train function and a test function, with dataloader as input. So, each function will go over the entire dataset with the dataloader, which loads data in batches.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pI4qSzR_KpV"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "def train_for_an_epoch(dataloader):\n",
        "  # put the model in train mode\n",
        "  model.train()\n",
        "\n",
        "  total_acc = 0.0\n",
        "  total_loss = 0.0\n",
        "\n",
        "  for text_batch, label_batch, length_batch in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(text_batch,length_batch)[:,0]\n",
        "    loss = loss_fn(pred, label_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "    total_loss += loss.item()*label_batch.size(0)\n",
        "\n",
        "  return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def test_for_an_epoch(dataloader):\n",
        "  model.eval()\n",
        "\n",
        "  total_acc = 0.0\n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for text_batch, label_batch, length_batch in dataloader:\n",
        "      pred = model(text_batch, length_batch)[:,0]\n",
        "      loss = loss_fn(pred, label_batch)\n",
        "\n",
        "      total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
        "      total_loss += loss.item()*label_batch.size(0)\n",
        "  \n",
        "  return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGaAFrZbC8YJ"
      },
      "source": [
        "*Let's do the training.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnIURLKCC_L0",
        "outputId": "8bdc7e85-c24e-40a0-963a-df4dc8db8f94"
      },
      "outputs": [],
      "source": [
        "#text_batch, label_batch, length_batch = next(iter(train_dl))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  acc_train, loss_train = train_for_an_epoch(train_dl)\n",
        "  acc_valid, loss_valid = test_for_an_epoch(valid_dl)\n",
        "\n",
        "  print(f'Epoch: {epoch}, train_accuracy: {acc_train:.4f}, val_accuracy: {acc_valid:.4f}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
